# Pod Naming Scheme

This guide explains Grove's hierarchical pod naming scheme and best practices for naming your resources. **Grove's naming convention is designed to be self-documenting**: when you run `kubectl get pods`, the pod names immediately tell you which PodCliqueSet, PodClique, and (if applicable) PodCliqueScalingGroup each pod belongs to, reflecting the complete hierarchy of your system.

## Why Hierarchical Naming Matters

Grove's naming scheme serves two critical purposes:

1. **Immediate Visual Understanding**: Pod names encode the complete hierarchy, so `kubectl get pods` output is self-explanatory. You can instantly see which pods belong together and how they're organized.

2. **Programmatic Service Discovery**: The hierarchical structure enables pods to discover and communicate with each other using fully qualified domain names (FQDNs). The [Environment Variables guide](./environment-variables.md) demonstrates how to programmatically construct these FQDNs using Grove's injected environment variables.

## Prerequisites

Before starting this guide:
- Review the [core concepts tutorial](./core-concepts/overview.md) to understand Grove's primitives
- Set up a cluster following the [installation guide](../installation.md), the two options are:
  - [A local KIND demo cluster](../installation.md#local-kind-cluster-set-up) Create the cluster with `make kind-up FAKE_NODES=40`, set `KUBECONFIG` env variable as directed, and run `make deploy`
  - [A remote Kubernetes cluster](../installation.md#remote-cluster-set-up) with [Grove installed from package](../installation.md#install-grove-from-package)

## Pod Naming Patterns

### Standalone PodCliques

For PodCliques that are **not** part of a PodCliqueScalingGroup, the pod naming follows this pattern:

```
<pcs-name>-<pcs-replica-index>-<pclq-name>-<random-suffix>
```

**Components:**
- `<pcs-name>`: The name of the PodCliqueSet
- `<pcs-replica-index>`: The replica index of the PodCliqueSet (0-based)
- `<pclq-name>`: The name of the PodClique template defined in the PodCliqueSet spec
- `<random-suffix>`: A random 5-character suffix generated by Kubernetes

**Example:** `multinode-disaggregated-0-frontend-a7b3c`

Looking at this name, you can immediately tell:
- It belongs to the `multinode-disaggregated` PodCliqueSet
- It's part of PodCliqueSet replica 0
- It's from the `frontend` PodClique

### PodCliques in a PodCliqueScalingGroup

For PodCliques that **are** part of a PodCliqueScalingGroup, the pod naming includes the PCSG information:

```
<pcs-name>-<pcs-replica-index>-<pcsg-name>-<pcsg-replica-index>-<pclq-name>-<random-suffix>
```

**Components:**
- `<pcs-name>`: The name of the PodCliqueSet
- `<pcs-replica-index>`: The replica index of the PodCliqueSet (0-based)
- `<pcsg-name>`: The name of the PodCliqueScalingGroup template
- `<pcsg-replica-index>`: The replica index of the PodCliqueScalingGroup (0-based)
- `<pclq-name>`: The name of the PodClique template within the PCSG
- `<random-suffix>`: A random 5-character suffix generated by Kubernetes

**Example:** `multinode-disaggregated-0-prefill-1-pworker-m9n0o`

Looking at this name, you can immediately tell:
- It belongs to the `multinode-disaggregated` PodCliqueSet (replica 0)
- It's part of the `prefill` PodCliqueScalingGroup (replica 1)
- It's from the `pworker` PodClique (prefill worker)

## Naming Best Practices

### Kubernetes Name Length Limit

Kubernetes has a **63-character limit** for resource names. Since Grove constructs full pod names by combining multiple components, you need to be mindful of name lengths when choosing names for your resources.

**How Grove constructs names:**

For standalone PodCliques, the final pod name is:
```
<pcs-name>-<pcs-replica-idx>-<pclq-name>-<5-char-suffix>
```

For PodCliques in a PCSG, the final pod name is:
```
<pcs-name>-<pcs-replica-idx>-<pcsg-name>-<pcsg-replica-idx>-<pclq-name>-<5-char-suffix>
```

**Character budget breakdown:**
- `<5-char-suffix>`: 5 characters (fixed by Kubernetes)
- `-` separators: 3-5 characters depending on structure
- Replica indices: 1+ characters each (single digit for 0-9, two digits for 10-99, etc.)
- Your chosen names: Remaining characters

### Naming Guidelines

1. **Use Short, Descriptive Names**: Choose concise but meaningful names
   - ✅ Good: `frontend`, `api`, `db`, `cache`
   - ❌ Avoid: `frontend-service-component`, `api-gateway-server`

2. **Use Abbreviations for Multi-Component Systems**: When you have multiple PodCliqueScalingGroups with similar roles, use prefixes or abbreviations
   - ✅ Good: `pleader`, `pworker` (prefill), `dleader`, `dworker` (decode)
   - ❌ Avoid: `prefill-leader`, `prefill-worker`, `decode-leader`, `decode-worker`

3. **Keep PodCliqueSet Names Short**: Remember that the PCS name is included in every pod name
   - ✅ Good: `ml-inference`, `web-app`, `data-pipeline`
   - ❌ Avoid: `machine-learning-inference-service`, `web-application-stack`

4. **Plan for Scaling**: Consider whether you'll need double-digit replica indices (adds 1 character per additional digit)
   - If you plan to scale to 10+ or 100+ or 1000+ replicas, budget accordingly

5. **Unique PodClique Names Within a PodCliqueSet**: All PodClique names must be unique within a PodCliqueSet. We explain the rationale for this further in [Why Unique PodClique Names Matter](#why-unique-podclique-names-matter)
   - If you have leader/worker patterns in multiple PCSGs, you **must** use different names (e.g., `pleader`/`pworker` and `dleader`/`dworker`)

### Example: Planning Names for a Complex System

Let's plan names for a multi-node disaggregated inference system with a frontend:

**Requirements:**
- 1 standalone frontend component
- 2 multi-node components: prefill and decode
- Each multi-node component has leader/worker roles
- We want to scale PCSGs to potentially 10+ replicas

**Name choices:**
- PodCliqueSet: `mn-disagg` (short, 9 chars)
- Standalone PodClique: `frontend` (8 chars)
- PCSG for prefill: `prefill` (7 chars)
  - Leader PodClique: `pleader` (7 chars)
  - Worker PodClique: `pworker` (7 chars)
- PCSG for decode: `decode` (6 chars)
  - Leader PodClique: `dleader` (7 chars)
  - Worker PodClique: `dworker` (7 chars)

**Resulting pod names:**
- Frontend: `mn-disagg-0-frontend-a7b3c` (28 chars) ✅
- Prefill leader: `mn-disagg-0-prefill-0-pleader-a7b3c` (36 chars) ✅
- Prefill worker: `mn-disagg-0-prefill-0-pworker-a7b3c` (36 chars) ✅
- Decode leader: `mn-disagg-0-decode-0-dleader-a7b3c` (34 chars) ✅
- Decode worker: `mn-disagg-0-decode-0-dworker-a7b3c` (34 chars) ✅

All names are well under the 63-character limit with room for growth!

## Hands-On Example: Multi-Node Disaggregated Inference

Let's deploy a realistic example that demonstrates both naming patterns and the requirement for unique PodClique names.

```yaml
apiVersion: grove.io/v1alpha1
kind: PodCliqueSet
metadata:
  name: multinode-disaggregated
  namespace: default
spec:
  replicas: 1
  template:
    cliques:
    # Standalone PodClique
    - name: frontend
      spec:
        replicas: 2
        podSpec:
          tolerations:
          - key: fake-node
            operator: Equal
            value: "true"
            effect: NoSchedule
          containers:
          - name: frontend
            image: nginx:latest
            command: ["/bin/sh"]
            args: ["-c", "echo 'Frontend' && hostname && sleep infinity"]
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
    # Prefill PodCliqueScalingGroup PodCliques
    - name: pleader
      spec:
        roleName: pleader
        replicas: 1
        podSpec:
          tolerations:
          - key: fake-node
            operator: Equal
            value: "true"
            effect: NoSchedule
          containers:
          - name: pleader
            image: nginx:latest
            command: ["/bin/sh"]
            args: ["-c", "echo 'Prefill Leader' && hostname && sleep infinity"]
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
    - name: pworker
      spec:
        roleName: pworker
        replicas: 3
        podSpec:
          tolerations:
          - key: fake-node
            operator: Equal
            value: "true"
            effect: NoSchedule
          containers:
          - name: pworker
            image: nginx:latest
            command: ["/bin/sh"]
            args: ["-c", "echo 'Prefill Worker' && hostname && sleep infinity"]
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
    # Decode PodCliqueScalingGroup PodCliques
    - name: dleader
      spec:
        roleName: dleader
        replicas: 1
        podSpec:
          tolerations:
          - key: fake-node
            operator: Equal
            value: "true"
            effect: NoSchedule
          containers:
          - name: dleader
            image: nginx:latest
            command: ["/bin/sh"]
            args: ["-c", "echo 'Decode Leader' && hostname && sleep infinity"]
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
    - name: dworker
      spec:
        roleName: dworker
        replicas: 2
        podSpec:
          tolerations:
          - key: fake-node
            operator: Equal
            value: "true"
            effect: NoSchedule
          containers:
          - name: dworker
            image: nginx:latest
            command: ["/bin/sh"]
            args: ["-c", "echo 'Decode Worker' && hostname && sleep infinity"]
            resources:
              requests:
                cpu: "10m"
                memory: "32Mi"
    podCliqueScalingGroups:
    - name: prefill
      cliqueNames: [pleader, pworker]
      replicas: 2
    - name: decode
      cliqueNames: [dleader, dworker]
      replicas: 1
```

### Why Unique PodClique Names Matter

Notice in the YAML above:
- Prefill PCSG uses `pleader` and `pworker` (not just `leader` and `worker`)
- Decode PCSG uses `dleader` and `dworker` (not just `leader` and `worker`)

**Why?**

PodClique names **must be unique within a PodCliqueSet**. Reusing generic names like `leader` and `worker` across multiple PodCliqueScalingGroups is therefore **not allowed**.

More importantly, this reflects **Grove’s core philosophy**.

In Grove, a `PodCliqueSet` is how you describe the **components of your system**. Each `PodClique` represents one component with a clearly defined, globally meaningful role. PodClique names are meant to be stable identifiers for those roles (e.g. prefill leader, decode worker), not local labels that change depending on how components are grouped or scaled.

When multiple components need to be co-scheduled, scaled together, and function as a single logical unit (e.g. a *super-pod* for prefill), they are placed into a `PodCliqueScalingGroup`. The scaling group defines *how components run together*, but it does not redefine or merge their identities.

That said, Grove also aims to keep names concise. Rather than naming a `PodClique` `prefill-leader`, we use short prefixes such as `p` (prefill) and `d` (decode) to preserve uniqueness while keeping names short and readable. The `PodCliqueScalingGroup` name already conveys which logical unit a PodClique belongs to, allowing the PodClique name itself to remain compact without losing clarity.

This is why `pleader`/`pworker` and `dleader`/`dworker` are the intended and recommended pattern.


### Deploy and Observe

In this example, we will deploy the file: [multinode-disaggregated-with-frontend.yaml](../../operator/samples/user-guide/naming-and-env-vars/multinode-disaggregated-with-frontend.yaml)

```bash
# NOTE: Run the following commands from the `/path/to/grove/operator` directory,
# where `/path/to/grove` is the root of your cloned Grove repository.
kubectl apply -f samples/user-guide/naming-and-env-vars/multinode-disaggregated-with-frontend.yaml

# Get all pods - observe the self-documenting names
kubectl get pods -l app.kubernetes.io/part-of=multinode-disaggregated -o wide
```

You should see output like:
```
NAME                                               READY   STATUS    RESTARTS   AGE
multinode-disaggregated-0-decode-0-dleader-abc12   1/1     Running   0          45s
multinode-disaggregated-0-decode-0-dworker-def34   1/1     Running   0          45s
multinode-disaggregated-0-decode-0-dworker-ghi56   1/1     Running   0          45s
multinode-disaggregated-0-frontend-jkl78           1/1     Running   0          45s
multinode-disaggregated-0-frontend-mno90           1/1     Running   0          45s
multinode-disaggregated-0-prefill-0-pleader-pqr12  1/1     Running   0          45s
multinode-disaggregated-0-prefill-0-pworker-stu34  1/1     Running   0          45s
multinode-disaggregated-0-prefill-0-pworker-vwx56  1/1     Running   0          45s
multinode-disaggregated-0-prefill-0-pworker-yza78  1/1     Running   0          45s
multinode-disaggregated-0-prefill-1-pleader-bcd90  1/1     Running   0          45s
multinode-disaggregated-0-prefill-1-pworker-efg12  1/1     Running   0          45s
multinode-disaggregated-0-prefill-1-pworker-hij34  1/1     Running   0          45s
multinode-disaggregated-0-prefill-1-pworker-klm56  1/1     Running   0          45s
```

### Parsing the Naming Hierarchy

Looking at this output, you can immediately understand the system structure:

**1. Standalone PodClique (frontend):**
```
multinode-disaggregated-0-frontend-*
```
- Simpler naming: `<pcs>-<pcs-idx>-<pclq>-<suffix>`
- 2 frontend pods serving requests

**2. PodCliqueScalingGroup (prefill) - 2 replicas:**
```
multinode-disaggregated-0-prefill-0-*
multinode-disaggregated-0-prefill-1-*
```
- Deeper hierarchy: `<pcs>-<pcs-idx>-<pcsg>-<pcsg-idx>-<pclq>-<suffix>`
- Each replica has 1 `pleader` + 3 `pworker` pods
- Two independent prefill clusters

**3. PodCliqueScalingGroup (decode) - 1 replica:**
```
multinode-disaggregated-0-decode-0-*
```
- Same deep hierarchy as prefill
- Has 1 `dleader` + 2 `dworker` pods
- One decode cluster

**4. Clear role identification through naming:**
- `frontend` = frontend component
- `pleader` = prefill leader
- `pworker` = prefill worker
- `dleader` = decode leader
- `dworker` = decode worker

### Examining Resources

Let's look at the underlying Grove resources:

```bash
# List PodCliques
kubectl get pclq
```

Output:
```
NAME                                       AGE
multinode-disaggregated-0-decode-0-dleader  2m
multinode-disaggregated-0-decode-0-dworker  2m
multinode-disaggregated-0-frontend          2m
multinode-disaggregated-0-prefill-0-pleader 2m
multinode-disaggregated-0-prefill-0-pworker 2m
multinode-disaggregated-0-prefill-1-pleader 2m
multinode-disaggregated-0-prefill-1-pworker 2m
```

**Observations:**
- Standalone PodClique: `multinode-disaggregated-0-frontend`
- Prefill PCSG PodCliques: Names include `prefill-0` or `prefill-1` to show which replica
- Decode PCSG PodCliques: Names include `decode-0`
- All names are unique and under 63 characters

```bash
# List PodCliqueScalingGroups
kubectl get pcsg
```

Output:
```
NAME                                AGE
multinode-disaggregated-0-decode    2m
multinode-disaggregated-0-prefill   2m
```

The PCSG names clearly identify the two scaling groups.

### Name Length Analysis

Let's verify our names fit within the 63-character limit:

- Longest pod name: `multinode-disaggregated-0-prefill-1-pworker-klm56`
  - Characters: 45 (well under 63) ✅

- PodClique names (max): `multinode-disaggregated-0-prefill-1-pworker`
  - Characters: 40 (under 63) ✅

- PCSG names: `multinode-disaggregated-0-prefill`
  - Characters: 32 (under 63) ✅

All resource names are comfortably within the limit!

### Cleanup

```bash
kubectl delete pcs multinode-disaggregated
```

---

## Resource Naming Reference

### Grove Resources and Their Naming

| Resource | You Name | Grove Generates | Pattern |
|----------|----------|-----------------|---------|
| **PodCliqueSet** | ✅ | - | `<your-pcs-name>` |
| **PodClique (template)** | ✅ | - | `<your-pclq-name>` (in spec.template.cliques) |
| **PCSG (template)** | ✅ | - | `<your-pcsg-name>` (in spec.template.podCliqueScalingGroups) |
| **PodClique (resource, standalone)** | - | ✅ | `<pcs-name>-<pcs-idx>-<pclq-name>` |
| **PodClique (resource, in PCSG)** | - | ✅ | `<pcs-name>-<pcs-idx>-<pcsg-name>-<pcsg-idx>-<pclq-name>` |
| **PCSG (resource)** | - | ✅ | `<pcs-name>-<pcs-idx>-<pcsg-name>` |
| **Pod (standalone)** | - | ✅ | `<pcs-name>-<pcs-idx>-<pclq-name>-<suffix>` |
| **Pod (in PCSG)** | - | ✅ | `<pcs-name>-<pcs-idx>-<pcsg-name>-<pcsg-idx>-<pclq-name>-<suffix>` |

**You control:** PodCliqueSet name, PodClique template names, PCSG template names  
**Grove generates:** All resource instances with hierarchical naming

## Key Takeaways

1. **Self-Documenting Hierarchy**: Pod names encode the complete hierarchy from PodCliqueSet → PCSG (if applicable) → PodClique → Pod, making `kubectl get pods` output immediately understandable.

2. **63-Character Limit**: Kubernetes enforces a 63-character limit on resource names. Use short, meaningful names for your resources, especially PodCliqueSet and PCSG names which appear in every generated name.

3. **Unique PodClique Names**: All PodClique names must be unique within a PodCliqueSet. When you have similar roles in multiple PCSGs (e.g., leader/worker in both prefill and decode), use prefixes or abbreviations (e.g., `pleader`/`pworker` and `dleader`/`dworker`).

4. **Predictable Patterns**: The naming scheme is consistent whether you're using standalone PodCliques or PodCliqueScalingGroups, making it easy to understand your system at a glance.

5. **Planning is Key**: Before creating resources, plan your names considering the full hierarchy and potential scaling needs.

## Next Steps

Now that you understand Grove's naming scheme and best practices, learn how to use these names programmatically in the [Environment Variables guide](./environment-variables.md), which shows:
- How Grove injects environment variables into your pods
- How to construct FQDNs for service discovery
- Practical examples of leader-worker communication
